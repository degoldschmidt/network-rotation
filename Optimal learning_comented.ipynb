{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gongui/miniconda2/envs/cnp1/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from numpy import *\n",
    "from numpy.random import randn, rand, randint\n",
    "\n",
    "from cmath import polar\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# for the following package you might have to install brewer2mpl first, than prettyplotlib\n",
    "# you can install both with the command\n",
    "# > sudo pip install brewer2mpl\n",
    "# > sudo pip install prettyplotlib\n",
    "import prettyplotlib as ppl\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the simulator that will do all the hard lifting: run the simulation, calculate spike times, integrate currents, change the weights, etc. If you want to change something about the learning, include the correction for the mean, etc., this is the place you have to edit.\n",
    "\n",
    "The code is written in Cython for reasons of speed. Cython converts Python code into C-code, the only thing you have to add are the types of the objects you define. Essentially all the funny parts of the code are related to this. In addition, you will notice that all the loops are written in index notation. This would be horribly slow in Python but actually helps the conversion into C-code. So, never vectorize anything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xrange(18)\n"
     ]
    }
   ],
   "source": [
    "print xrange(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "from libc.math cimport log, exp, sqrt, sin\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "def c_analytic_opt_simulate(np.ndarray[double, ndim=2] Wf, np.ndarray[double, ndim=1] T, np.ndarray[double, ndim=2] F, \n",
    "                        np.ndarray[double, ndim=1] I_int, np.ndarray[double, ndim=1] C_int, np.ndarray[double, ndim=1] V, \n",
    "                        double yd, float o, double mu1, double mu2, double inpDt, double recDt, double Rstep, \n",
    "                        double Fstep, double Ustep, double t0, unsigned int sim_T):\n",
    "    '''\n",
    "\n",
    "       PARAMETERS\n",
    "       ----------\n",
    "       Wf    - fast reconnect connections\n",
    "       T     - Thresholds\n",
    "       F     - feedforward connections\n",
    "       I_int - the initial value of the integrated input current (vector of size N (# neurons)). Of minor importance if you restart a simulation.\n",
    "       C_int - same as I_int but for the true control current (vector of size I (# input dimensions))\n",
    "       V     - initial membrane voltage\n",
    "       o     - std of noise\n",
    "       mu1   - L1 constraint\n",
    "       mu2   - L2 constraint\n",
    "       inpDt - the input current is constant over a period of time and switches regularly with frequency f = 1/inpDt\n",
    "       recDt - we collect a couple of variables over the time course of the simulation with a sampling frequency 1/recDt\n",
    "       Rstep - learning rate of lateral connections\n",
    "       Fstep - learning rate of feedforward connections\n",
    "       t0    - intial time (important if you restart the simulation for the timestamps)\n",
    "       sim_T - total simulation time\n",
    "\n",
    "    '''\n",
    "    # basic parameters of the network\n",
    "    cdef unsigned int N = Wf.shape[0]                 # number of neurons\n",
    "    cdef unsigned int J = F.shape[0]                  # number of inputs                \n",
    "    cdef double t = t0                                # time\n",
    "    \n",
    "    # containers for data collections\n",
    "    Wfdata = list()                                   # container for state of lateral weights\n",
    "    Fdata = list()                                    # container for state of feedforward weights\n",
    "    tdata = list()                                    # time-stamps\n",
    "    ratedata = list()                                 # rates  \n",
    "\n",
    "    cdef double last_rec_t = t0                       # last time data was collected\n",
    "    \n",
    "    # normalization of feedforward weights\n",
    "    cdef double Fnorm = sqrt(sum(F[:,0]**2))\n",
    "    \n",
    "    # spike times\n",
    "    cdef np.ndarray[double, ndim=1] s = np.empty(N)    # time of spike of neurons (if they would be in isolation)\n",
    "    cdef double snext                                  # time to next spike in population (minimum of s)\n",
    "\n",
    "    # input-related variables\n",
    "    cdef np.ndarray[double, ndim=1] c = abs(np.random.randn(J))   # control current (size J)\n",
    "    cdef np.ndarray[double, ndim=1] Fc = np.dot(F.T,c)            # input current (size N)\n",
    "    \n",
    "    # other dynamic variables to track\n",
    "    cdef np.ndarray[double, ndim=1] rate = np.zeros(N)\n",
    "    cdef np.ndarray[double, ndim=1] r = np.zeros(N)\n",
    "    cdef double totr = 0\n",
    "    cdef np.ndarray[double, ndim=1] Fu = np.zeros(N) \n",
    "    cdef np.ndarray[double, ndim=1] x = np.zeros(J)\n",
    "    cdef double Fx = 0\n",
    "    cdef double Wr = 0\n",
    "    \n",
    "    # auxiliary variables\n",
    "    cdef unsigned int i, j\n",
    "    cdef double norm = 0\n",
    "    cdef double divisor = 0\n",
    "    cdef double logarg = 0\n",
    "    \n",
    "    # simulation specific variables\n",
    "    #   the simulation analytically computes the spike-times and steps to the next neuron that spikes\n",
    "    #   if this time is larger than no_spike_dt (e.g. if no neuron would spike because the leak is too strong\n",
    "    #   and the signal to weak), than no spike will be emited and the simulation steps no_spike_dt forward\n",
    "    cdef double no_spike_dt = 10\n",
    "    \n",
    "    #   connected auxiliary variables\n",
    "    cdef unsigned int spike_flag = 0    # auxiliary variables that signals if there was a spike in a simulation step\n",
    "    cdef double last_input_t = 0        # last time the input changed\n",
    "    \n",
    "    # counts the number of spikes from beginning of simulation\n",
    "    cdef unsigned int spikenum = 0\n",
    "    \n",
    "    # signal progress\n",
    "    cdef double progress_update_dt = sim_T/1000\n",
    "    cdef double last_progress_update_t = 0\n",
    "    \n",
    "    while t < sim_T + t0:   \n",
    "        # compute the next spike-times\n",
    "        snext, i = no_spike_dt, 0\n",
    "        spike_flag = 0\n",
    "                \n",
    "        for j in xrange(N):\n",
    "            # if (for whatever reason) the voltage is above the threshold right now, make the neuron spike\n",
    "            if V[j] > T[j] - Fu[j]:\n",
    "                snext, i = 0, j\n",
    "                break\n",
    "            else:\n",
    "                # compute the time at which the membrane voltage would hit the threshold (spike-time)\n",
    "                # you have to be careful not to run in numerical issues\n",
    "                divisor = Fc[j] - yd*(T[j] - Fu[j])\n",
    "                if abs(divisor) < 1e-10:\n",
    "                    divisor = np.sign(divisor)*1e-10\n",
    "                \n",
    "                logarg = (Fc[j] - yd*V[j])/divisor\n",
    "                \n",
    "                if logarg >= 1:   # neuron would spike in the future\n",
    "                    s[j] = 1/yd*log(logarg)    # spike time of neuron j\n",
    "                    # check if spike time is lower than all previously computed spike times\n",
    "                    if s[j] < snext:\n",
    "                        snext, i = s[j], j\n",
    "                        spike_flag = 1      # yes, there is a neuron that would like to spike!\n",
    "        \n",
    "        # calculate voltage at time of spike & propagate spike to the network\n",
    "        for j in xrange(N):\n",
    "            V[j] = Fc[j]/yd + exp(-yd*snext)*(V[j] - Fc[j]/yd)# + sqrt(snext)*o*np.random.randn()\n",
    "\n",
    "        # propagate spikes\n",
    "        if spike_flag == 1:\n",
    "            for n in xrange(N):\n",
    "                V[n] -= Wf[n,i]\n",
    "\n",
    "        # update x and xhat\n",
    "        for j in xrange(J):\n",
    "            x[j] = c[j]/yd + exp(-yd*snext)*(x[j] - c[j]/yd)\n",
    "                \n",
    "        # update rate\n",
    "        for n in xrange(N):\n",
    "            r[n] = exp(-yd*snext)*r[n]\n",
    "        \n",
    "        totr = exp(-yd*snext)*totr\n",
    "            \n",
    "        if spike_flag:\n",
    "            r[i] += 1\n",
    "            totr += 1\n",
    "            \n",
    "        # update u\n",
    "        for n in xrange(N):\n",
    "            Fx = 0\n",
    "            for j in xrange(J):\n",
    "                Fx += F[j,n]*x[j]\n",
    "                \n",
    "            Wr = 0\n",
    "            for m in xrange(N):\n",
    "                Wr += Wf[n,m]*r[m]\n",
    "                \n",
    "            Fu[n] += Ustep*(Fx - Wr - Fu[n])\n",
    "\n",
    "        # integrate membrane currents\n",
    "        for n in xrange(N):\n",
    "            #I_int[n] += Fc[n]*snext                                     # without leak\n",
    "            I_int[n] = Fc[n]/yd + exp(-yd*snext)*(I_int[n] - Fc[n]/yd)   # with leak\n",
    "            \n",
    "\n",
    "        # input input currents\n",
    "        for j in xrange(J):\n",
    "            C_int[j] += c[j]*snext                                               # without leak\n",
    "            #C_int[j] += c[j]/I_lam + (C_int[j] - c[j]/I_lam)*exp(-I_lam*snext)  # with leak\n",
    "\n",
    "        # update recurrent weights (but wait some spikes to avoid init effects)        \n",
    "        if spikenum > 100:\n",
    "            if spike_flag:\n",
    "                for n in xrange(N):\n",
    "                    if n != i:\n",
    "                        Wf[n,i] += Rstep*(I_int[n] - Wf[n,i] - mu2*r[n]/totr)\n",
    "\n",
    "        # update feedforward weights\n",
    "        if spikenum > 100:\n",
    "            if spike_flag == 1:\n",
    "                for j in xrange(J):\n",
    "                    F[j,i] += Fstep*(C_int[j] - F[j,i])\n",
    "                    #F[j,i] += Fstep*(c[j] - F[j,i])\n",
    "                    \n",
    "                # normalize\n",
    "                norm = 0\n",
    "                for j in xrange(J):\n",
    "                    norm += F[j,i]**2\n",
    "                    \n",
    "                norm = sqrt(norm)\n",
    "                \n",
    "                for j in xrange(J):\n",
    "                    F[j,i] *= Fnorm/norm\n",
    "                    \n",
    "                # recompute product Fc (for given column)\n",
    "                Fc[i] = 0\n",
    "                for j in xrange(J):\n",
    "                    Fc[i] += F[j,i]*c[j]\n",
    "\n",
    "        # periodically set input\n",
    "        if t - last_input_t > inpDt:\n",
    "            \n",
    "            # randomly draw input\n",
    "            for j in xrange(J):\n",
    "                #c[j] = abs(np.random.randn())\n",
    "                c[j] = np.random.rand()*3.464\n",
    "\n",
    "            # recompute Fc\n",
    "            for n in xrange(N):\n",
    "                Fc[n] = 0\n",
    "                for j in xrange(J):\n",
    "                    Fc[n] += F[j,n]*c[j]\n",
    "            \n",
    "            last_input_t = t\n",
    "        \n",
    "        # set new time\n",
    "        t = t + snext\n",
    "        \n",
    "        # increase the spike count\n",
    "        if spike_flag:\n",
    "            spikenum = spikenum + 1\n",
    "        \n",
    "        # reset current counter if one neuron spiked\n",
    "        if spike_flag == 1:\n",
    "            for j in xrange(J):\n",
    "                C_int[j] = 0\n",
    "                \n",
    "            for n in xrange(N):\n",
    "                I_int[n] = 0\n",
    "        \n",
    "        # set last spike-time\n",
    "        if spike_flag == 1:\n",
    "            last_spike_t = t\n",
    "        \n",
    "        # collect spike-numbers\n",
    "        if spike_flag == 1:\n",
    "            rate[i] += 1\n",
    "        \n",
    "        # collect data\n",
    "        if t - last_rec_t > recDt:\n",
    "            Wfdata.append(Wf.copy())\n",
    "            Fdata.append(F.copy())            \n",
    "            tdata.append(t)\n",
    "            ratedata.append(rate.copy()/float(recDt))\n",
    "            for n in xrange(N):\n",
    "                rate[n] = 0\n",
    "            last_rec_t = t\n",
    "            \n",
    "        # print progress\n",
    "        if t - last_progress_update_t > progress_update_dt:\n",
    "            print '\\r', np.around((t-t0)/sim_T*100,1), '%',\n",
    "            last_progress_update_t = t\n",
    "           \n",
    "    return V, Wf, F, I_int, C_int, t, Wfdata, Fdata, tdata, ratedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruction Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next simulation is only to capture the reconstruction error of a given set of parameters. It is less commented since it is very similar to the simulation above and you should not have to change anything here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "cimport cython\n",
    "cimport numpy as np\n",
    "from libc.math cimport log, exp, sqrt, sin\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "@cython.cdivision(True)\n",
    "def performance(np.ndarray[double, ndim=2] Wf, np.ndarray[double, ndim=1] T, np.ndarray[double, ndim=2] F, \n",
    "                double yd, double mu1, double mu2, double inpDt, unsigned int sim_T):\n",
    "\n",
    "    cdef unsigned int N = Wf.shape[0]                 # number of neurons\n",
    "    cdef unsigned int J = F.shape[0]                  # number of inputs\n",
    "    cdef unsigned int spike\n",
    "    \n",
    "    cdef double t = 0\n",
    "    cdef double no_spike_dt = 10\n",
    "    cdef unsigned int spike_flag = 0\n",
    "    cdef double last_spike_t = 0\n",
    "    cdef double divisor = 0\n",
    "    cdef double logarg = 0\n",
    "    cdef double last_input_t = 0                  # remember the last time the input changed\n",
    "    cdef double last_rec_t = 0                # remember the last time data was collected\n",
    "    cdef double snext\n",
    "    cdef unsigned int i, j\n",
    "    cdef unsigned int spikenum = 0                # counts the number of spikes\n",
    "    \n",
    "    cdef double progress_update_dt = sim_T/1000\n",
    "    cdef double last_progress_update_t = 0\n",
    "     \n",
    "    cdef np.ndarray[double, ndim=1] rate = np.zeros(N)\n",
    "    \n",
    "    cdef np.ndarray[double, ndim=1] c = abs(np.random.randn(J))\n",
    "    cdef np.ndarray[double, ndim=1] Fc = np.dot(F.T,c)\n",
    "    cdef np.ndarray[double, ndim=1] s = np.empty(N)\n",
    "    cdef np.ndarray[double, ndim=1] r = np.empty(N)\n",
    "    \n",
    "    cdef np.ndarray[double, ndim=1] x = np.zeros(J)    \n",
    "    cdef np.ndarray[double, ndim=1] xhat = np.zeros(J)\n",
    "    cdef np.ndarray[double, ndim=1] V = np.zeros(N)    \n",
    "    \n",
    "    cdef int cumrate = 0\n",
    "    cdef double cumerror = 0\n",
    "    cdef double cumr2 = 0\n",
    "    cdef double cumr1 = 0\n",
    "    \n",
    "    # compute optimal decoder\n",
    "    cdef np.ndarray[double, ndim=2] D = np.dot(np.linalg.pinv(F).T,Wf)\n",
    "    \n",
    "    while t < sim_T:   \n",
    "        # compute the next spike-times\n",
    "        snext, i = no_spike_dt, 0\n",
    "        \n",
    "        for j in xrange(N):\n",
    "            # test argument of logarithm\n",
    "            divisor = Fc[j] - yd*T[j]\n",
    "            if abs(divisor) < 1e-10:\n",
    "                divisor = 1e-10\n",
    "            \n",
    "            logarg = (Fc[j] - yd*V[j])/divisor\n",
    "            \n",
    "            if logarg > 1:\n",
    "                s[j] = 1/yd*log(logarg)\n",
    "                if s[j] < snext:\n",
    "                    snext, i = s[j], j\n",
    "        \n",
    "        # if no neuron spikes raise flag\n",
    "        if snext == no_spike_dt:\n",
    "            spike_flag = 0\n",
    "        else:\n",
    "            spike_flag = 1\n",
    "        \n",
    "        # calculate rate\n",
    "        if spike_flag:\n",
    "            cumrate += 1\n",
    "\n",
    "        # calculate error\n",
    "        for j in xrange(J):           \n",
    "            error = 2*c[j]*(exp(yd*snext)-1)**2*(x[j] - xhat[j])*yd\n",
    "            error += (exp(2*yd*snext) - 1)*(x[j] - xhat[j])**2*yd**2\n",
    "            error += c[j]**2*(4*exp(yd*snext) - 1 + exp(2*yd*snext)*(2*yd*snext - 3))\n",
    "            cumerror += exp(-2*snext*yd)*error/(2*yd**3)\n",
    "            \n",
    "        for n in xrange(N):\n",
    "            cumr2 += (1 - exp(-2*yd*snext))*r[n]**2/(2*yd)\n",
    "            cumr1 += (1 - exp(-yd*snext))*r[n]/yd\n",
    "            \n",
    "        # update x and xhat\n",
    "        for j in xrange(J):\n",
    "            x[j] = c[j]/yd + exp(-yd*snext)*(x[j] - c[j]/yd)\n",
    "            xhat[j] = exp(-yd*snext)*xhat[j]\n",
    "            \n",
    "            if spike_flag:\n",
    "                xhat[j] += D[j,i]\n",
    "                \n",
    "        # update rate\n",
    "        for n in xrange(N):\n",
    "            r[n] = exp(-yd*snext)\n",
    "            \n",
    "        if spike_flag:\n",
    "            r[i] += 1\n",
    "                \n",
    "        # calculate voltage at time of spike & propagate spike to the network\n",
    "        for j in xrange(N):\n",
    "            V[j] = Fc[j]/yd + exp(-yd*snext)*(V[j] - Fc[j]/yd)# + sqrt(snext)*o*np.random.randn()\n",
    "            \n",
    "            # avoid that noises pushes a neuron above the threshold\n",
    "            if V[j] > T[j]:\n",
    "                V[j] = T[j]\n",
    "            \n",
    "        # fix voltage of firing neuron to threshold (if it was changed by noise)\n",
    "        if spike_flag == 1:\n",
    "            V[i] = T[i]\n",
    "        \n",
    "        # propagate spikes\n",
    "        if spike_flag == 1:\n",
    "            for n in xrange(N):\n",
    "                V[n] -= Wf[n,i]\n",
    "\n",
    "        # periodically set input\n",
    "        if t - last_input_t > inpDt:\n",
    "            \n",
    "            # randomly draw input\n",
    "            for j in xrange(J):\n",
    "                #c[j] = abs(np.random.randn())\n",
    "                c[j] = np.random.rand()*3.464\n",
    "\n",
    "            # recompute Fc\n",
    "            for n in xrange(N):\n",
    "                Fc[n] = 0\n",
    "                for j in xrange(J):\n",
    "                    Fc[n] += F[j,n]*c[j]\n",
    "            \n",
    "            last_input_t = t\n",
    "        \n",
    "        # set new time\n",
    "        t = t + snext\n",
    "        \n",
    "        # print progress\n",
    "        if t - last_progress_update_t > progress_update_dt:\n",
    "            #print x - xhat\n",
    "            print '\\r', np.around(t/sim_T*100,1), '%',\n",
    "            last_progress_update_t = t\n",
    "            \n",
    "    return cumrate/(float(sim_T)*N), cumerror/float(sim_T), cumr1/float(sim_T), cumr2/float(sim_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_performance(system,T=1000000):\n",
    "    return performance(system.Wf, system.T, system.F, system.yd, system.mu1, system.mu2, system.inpDt, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FD_distance(system):\n",
    "    Wf = system.Wf - eye(system.Wf.shape[0])*system.mu2\n",
    "    D = dot(linalg.pinv(system.F).T,Wf).T\n",
    "    return sum((Wf - dot(system.F.T,D.T))**2)/sum(Wf**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator object essentially stores all relevant variables of the system. Think of it is the \"network object\". It stores the thresholds, weights, collected data, L1/L2 constraints, etc. We will first initialize this object with all relevant parameters and than call its function run(), which in turn calls the simulation function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class simulator(object):\n",
    "    \n",
    "    def __init__(self,Wf,T,F,yd,o,inpDt,mode='opt'):\n",
    "        # network structure\n",
    "        self.Wf = Wf   # lateral\n",
    "        self.F = F     # feedforward\n",
    "        self.T = T     # threshold\n",
    "        self.yd = yd   # leak\n",
    "        self.mu2 = 0.\n",
    "        self.mu1 = 0.\n",
    "        \n",
    "        # dynamical parameters\n",
    "        self.o = o     # std of noise\n",
    "        self.I_int = zeros_like(T)                     # state of integrated input current\n",
    "        self.C_int = zeros(F.shape[0])                 # state of integrated control current\n",
    "        self.V = zeros_like(T) + 1e-5*randn(*T.shape)  # state of voltage\n",
    "        \n",
    "        # input parameters\n",
    "        self.inpDt = inpDt\n",
    "        \n",
    "        # recorded data        \n",
    "        self.rec_t = array([0])\n",
    "        self.rec_Wf = Wf.copy()\n",
    "        self.rec_F = F.copy()\n",
    "        self.rec_rate = zeros_like(T)\n",
    "        self.rec_ISI = array([0])\n",
    "        \n",
    "        # simulation/plasticity parameters\n",
    "        self.t = 0\n",
    "        self.Rstep = 1e-5    # default learning step for laterals\n",
    "        self.Fstep = 1e-6    # default learning step for feedforward\n",
    "        self.Ustep = 1e-6    # default learning step for feedforward        \n",
    "        self.recDt = 100     # default distance between data recordings \n",
    "        \n",
    "    def run(self,Tsim):\n",
    "        if not hasattr(self, 'rec_Rstep'):\n",
    "            self.rec_Rstep = array([self.Rstep])\n",
    "        \n",
    "        # run simulation\n",
    "        V, Wf, F, I_int, C_int, t, Wf_data, F_data, t_data, rate_data = c_analytic_opt_simulate(self.Wf,\\\n",
    "                self.T,self.F,self.I_int,self.C_int,self.V,self.yd,self.o,self.mu1,self.mu2,self.inpDt,\\\n",
    "                self.recDt,self.Rstep,self.Fstep,self.Ustep,self.t,Tsim)\n",
    "\n",
    "        # update internal values for next run\n",
    "        self.V[:] = V\n",
    "        self.Wf[:,:] = Wf\n",
    "        self.F[:,:] = F\n",
    "        self.I_int[:] = I_int\n",
    "        self.C_int[:] = C_int\n",
    "        self.t = t\n",
    "        \n",
    "        # update stored values\n",
    "        t_data = array(t_data)\n",
    "        self.rec_t = hstack([self.rec_t,t_data])\n",
    "        self.rec_rate = vstack([self.rec_rate.T,rate_data]).T        \n",
    "        \n",
    "        Wf_data = dstack(Wf_data)\n",
    "        F_data = dstack(F_data)        \n",
    "        self.rec_Wf = dstack([self.rec_Wf,Wf_data])\n",
    "        self.rec_F = dstack([self.rec_F,F_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NETWORK VARIABLES\n",
    "N    = 5                    # number of Neurons in Network\n",
    "I    = 2                     # dimension of input\n",
    "yd   = 0.005                  # decoder timescale (in milliseconds)\n",
    "beta = 0.05                    # quadratic constraint\n",
    "o    = 1e-8                  # std of voltage noise\n",
    "    \n",
    "# INIT RECURRENT & FEEDFORWARD CONNECTIVITY\n",
    "F = ones((I,N))/float(N)     # Initilise Readout connectivity\n",
    "F = rand(*(I,N))\n",
    "F /= sqrt(sum(F**2,axis=0))[None,:]\n",
    "#F /= sqrt(float(N))\n",
    "\n",
    "F_origin = F.copy()\n",
    "Fnorm = sqrt(sum(F[:,0]**2))\n",
    "\n",
    "# save optimal recurrent weights for later use\n",
    "opt_W = dot(F.T,F)\n",
    "\n",
    "# random initial lateral connectivity\n",
    "Wf = 0.5*rand(*opt_W.shape)*opt_W\n",
    "\n",
    "# set correct resets (diagonal of Wf)\n",
    "Wf[np.diag_indices_from(Wf)] = diag(opt_W) + beta\n",
    "\n",
    "# INIT THRESHOLD\n",
    "T = diag(opt_W)/2. + beta/2.\n",
    "\n",
    "sim_T = 1000\n",
    "Dt = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize network\n",
    "system = simulator(Wf.copy(),T.copy(),F.copy(),yd,o,inpDt=10,mode='opt')\n",
    "\n",
    "# some data collections\n",
    "errors = []\n",
    "times = []\n",
    "objective = []\n",
    "FD = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# learning rate of lateral\n",
    "system.Rstep = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tsim = 10000000\n",
    "\n",
    "# number of times during simulation in which to test reconstruction performance\n",
    "Ntest = 10\n",
    "\n",
    "# interval between two tests\n",
    "sim_T = Tsim/float(Ntest)\n",
    "\n",
    "# start simulation\n",
    "for trial in xrange(Ntest):\n",
    "    print trial\n",
    "    \n",
    "    # compute reconstruction error\n",
    "    rate, error, r1, r2 = get_performance(system,T=100000)\n",
    "    errors.append(error)\n",
    "    times.append(system.t)\n",
    "    objective.append(error + system.mu1*r1 + system.mu2*r2)\n",
    "    \n",
    "    # compute distance to FD\n",
    "    FD.append(FD_distance(system))\n",
    "    \n",
    "    \n",
    "    # parameters\n",
    "    system.yd = 0.0005\n",
    "    system.o = 1e-7\n",
    "    system.recDt = sim_T/5\n",
    "    #system.Rstep = 0.5*1e-5\n",
    "    system.Fstep = 1e-6\n",
    "    system.mu1 = 0*beta\n",
    "    system.mu2 = beta\n",
    "    \n",
    "    # train system\n",
    "    system.run(sim_T)\n",
    "    \n",
    "    print 'finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(18,10))\n",
    "subplots_adjust(hspace=0.3)\n",
    "\n",
    "suptitle('5 neurons / 2 inputs / positive / feedforward / low leak / no L1 / medium L2 / ', fontsize=16)\n",
    "\n",
    "# color\n",
    "syscol = cm.Blues(0.9)\n",
    "\n",
    "labels = 'Optimal'\n",
    "\n",
    "def find_time_slice(t_start,t_end,time_axis):\n",
    "    if t_end == -1:\n",
    "        i_end = -1\n",
    "    else:\n",
    "        i_end = searchsorted(time_axis,t_end)\n",
    "        \n",
    "    i_start = searchsorted(time_axis,t_start)\n",
    "    return i_start, i_end\n",
    "\n",
    "#start, end = 500000, -1\n",
    "time = {}\n",
    "t_start, t_end = 0, -1\n",
    "\n",
    "start, end = find_time_slice(t_start,t_end,system.rec_t)\n",
    "time = system.rec_t/float(1e7)\n",
    "\n",
    "# add plot of population rate\n",
    "\n",
    "subplot(231)\n",
    "title('Distance to optimal recurrent weights')\n",
    "\n",
    "def get_distance(system):\n",
    "    Wf_opt = einsum('kit,kjt->ijt',system.rec_F[:,:,start:end],system.rec_F[:,:,start:end]) + eye(N)[:,:,None]*system.mu2\n",
    "    distance =  sum(sum((system.rec_Wf[:,:,start:end] - Wf_opt)**2,axis=0),axis=0)/sum(sum(Wf_opt**2,axis=0),axis=0)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "distance = get_distance(system)\n",
    "ppl.plot(time[start:end],distance,linewidth=3, color=syscol)\n",
    "\n",
    "xlabel('time [1e7]')\n",
    "#gca().set_yscale('log')\n",
    "\n",
    "subplot(232)\n",
    "title('Population rate')\n",
    "\n",
    "def get_pop_rate(system):\n",
    "    poprate = mean(system.rec_rate[:,start+1:end],axis=0)\n",
    "    poprate = gaussian_filter1d(poprate,poprate.shape[0]/50)\n",
    "\n",
    "    return poprate\n",
    "\n",
    "poprate = get_pop_rate(system)\n",
    "ppl.plot(time[start+1:end],poprate,linewidth=3,c=syscol)\n",
    "\n",
    "for n in xrange(N):\n",
    "    nrate = system.rec_rate[n,start+1:end]\n",
    "    nrate = gaussian_filter1d(nrate,nrate.shape[0]/50)\n",
    "    ppl.plot(time[start+1:end],nrate,linewidth=1,c=syscol,alpha=0.3)\n",
    "        \n",
    "xlabel('time [1e7]')\n",
    "\n",
    "subplot(233)\n",
    "title('Optimal vs Learned weights')\n",
    "\n",
    "opt_W = dot(system.F.T,system.F) + system.mu2*eye(system.F.shape[1])\n",
    "ppl.scatter(opt_W.flatten(),system.Wf.flatten(),facecolor=syscol)\n",
    "\n",
    "subplot(234)\n",
    "title('Reconstruction error')\n",
    "\n",
    "ppl.plot(array(times[1:])/float(1e7),errors[1:],linewidth=3, color=syscol, label=labels)\n",
    "\n",
    "xlabel('time [1e7]')\n",
    "ax = gca()\n",
    "\n",
    "subplot(235)\n",
    "title('Distance to FD')\n",
    "\n",
    "ppl.plot(array(times[1:])/float(1e7),FD[1:],linewidth=3, color=syscol)\n",
    "\n",
    "xlabel('time [1e7]')\n",
    "\n",
    "# draw legend\n",
    "lg = figlegend(*ax.get_legend_handles_labels(),loc='lower right')\n",
    "lg.draw_frame(False)\n",
    "\n",
    "subplot(236,polar=True)\n",
    "title('Receptive Fields')\n",
    "\n",
    "for n in xrange(N):\n",
    "    f = system.F[:,n]\n",
    "    r, theta = polar(f[0] + f[1]*1j)\n",
    "    ppl.plot([0,theta],[0,r],c=syscol,linewidth=1.5)\n",
    "\n",
    "tick_params(\\\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off', # labels along the bottom edge are off\n",
    "    labelleft='off',\n",
    "    labelright='off',\n",
    "    labeltop='off',\n",
    "    left='off',\n",
    "    right='off')\n",
    "\n",
    "# shift the axis a bit down\n",
    "bbox=gca().get_position()\n",
    "gca().set_position([bbox.x0, bbox.y0-0.02, bbox.x1-bbox.x0, bbox.y1-bbox.y0])\n",
    "\n",
    "#savefig('positive_5_2_feedforward_mediumL2_lowLeak.pdf', bbox_inches='tight')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(18,10))\n",
    "subplots_adjust(hspace=0.3)\n",
    "\n",
    "suptitle('5 neurons / 2 inputs / positive / feedforward / low leak / no L1 / medium L2 / ', fontsize=16)\n",
    "\n",
    "# color\n",
    "syscol = cm.Blues(0.9)\n",
    "\n",
    "labels = 'Optimal'\n",
    "\n",
    "def find_time_slice(t_start,t_end,time_axis):\n",
    "    if t_end == -1:\n",
    "        i_end = -1\n",
    "    else:\n",
    "        i_end = searchsorted(time_axis,t_end)\n",
    "        \n",
    "    i_start = searchsorted(time_axis,t_start)\n",
    "    return i_start, i_end\n",
    "\n",
    "#start, end = 500000, -1\n",
    "time = {}\n",
    "t_start, t_end = 0, -1\n",
    "\n",
    "start, end = find_time_slice(t_start,t_end,system.rec_t)\n",
    "time = system.rec_t/float(1e7)\n",
    "\n",
    "# add plot of population rate\n",
    "\n",
    "subplot(231)\n",
    "title('Distance to optimal recurrent weights')\n",
    "\n",
    "def get_distance(system):\n",
    "    Wf_opt = einsum('kit,kjt->ijt',system.rec_F[:,:,start:end],system.rec_F[:,:,start:end]) + eye(N)[:,:,None]*system.mu2\n",
    "    distance =  sum(sum((system.rec_Wf[:,:,start:end] - Wf_opt)**2,axis=0),axis=0)/sum(sum(Wf_opt**2,axis=0),axis=0)\n",
    "    \n",
    "    return distance\n",
    "\n",
    "distance = get_distance(system)\n",
    "ppl.plot(time[start:end],distance,linewidth=3, color=syscol)\n",
    "\n",
    "xlabel('time [1e7]')\n",
    "#gca().set_yscale('log')\n",
    "\n",
    "subplot(232)\n",
    "title('Population rate')\n",
    "\n",
    "def get_pop_rate(system):\n",
    "    poprate = mean(system.rec_rate[:,start+1:end],axis=0)\n",
    "    poprate = gaussian_filter1d(poprate,poprate.shape[0]/50)\n",
    "\n",
    "    return poprate\n",
    "\n",
    "poprate = get_pop_rate(system)\n",
    "ppl.plot(time[start+1:end],poprate,linewidth=3,c=syscol)\n",
    "\n",
    "for n in xrange(N):\n",
    "    nrate = system.rec_rate[n,start+1:end]\n",
    "    nrate = gaussian_filter1d(nrate,nrate.shape[0]/50)\n",
    "    ppl.plot(time[start+1:end],nrate,linewidth=1,c=syscol,alpha=0.3)\n",
    "        \n",
    "xlabel('time [1e7]')\n",
    "\n",
    "subplot(233)\n",
    "title('Optimal vs Learned weights')\n",
    "\n",
    "opt_W = dot(system.F.T,system.F) + system.mu2*eye(system.F.shape[1])\n",
    "ppl.scatter(opt_W.flatten(),system.Wf.flatten(),facecolor=syscol)\n",
    "\n",
    "subplot(234)\n",
    "title('Reconstruction error')\n",
    "\n",
    "ppl.plot(array(times[1:])/float(1e7),errors[1:],linewidth=3, color=syscol, label=labels)\n",
    "\n",
    "xlabel('time [1e7]')\n",
    "ax = gca()\n",
    "\n",
    "subplot(235)\n",
    "title('Distance to FD')\n",
    "\n",
    "ppl.plot(array(times[1:])/float(1e7),FD[1:],linewidth=3, color=syscol)\n",
    "\n",
    "xlabel('time [1e7]')\n",
    "\n",
    "# draw legend\n",
    "lg = figlegend(*ax.get_legend_handles_labels(),loc='lower right')\n",
    "lg.draw_frame(False)\n",
    "\n",
    "subplot(236,polar=True)\n",
    "title('Receptive Fields')\n",
    "\n",
    "for n in xrange(N):\n",
    "    f = system.F[:,n]\n",
    "    r, theta = polar(f[0] + f[1]*1j)\n",
    "    ppl.plot([0,theta],[0,r],c=syscol,linewidth=1.5)\n",
    "\n",
    "tick_params(\\\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off', # labels along the bottom edge are off\n",
    "    labelleft='off',\n",
    "    labelright='off',\n",
    "    labeltop='off',\n",
    "    left='off',\n",
    "    right='off')\n",
    "\n",
    "# shift the axis a bit down\n",
    "bbox=gca().get_position()\n",
    "gca().set_position([bbox.x0, bbox.y0-0.02, bbox.x1-bbox.x0, bbox.y1-bbox.y0])\n",
    "\n",
    "#savefig('positive_5_2_feedforward_mediumL2_lowLeak.pdf', bbox_inches='tight')\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
