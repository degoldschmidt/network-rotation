\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{Marr1982}
\citation{Adrian1926}
\citation{Ernst2002,Behrens2007,Drugowitsch2014}
\citation{Attneave1954,Barlow1961}
\citation{Shannon1948}
\citation{Olshausen1996,Bell1997}
\citation{Atick1992,Pitkow2012}
\citation{Srinivasan1982,VanHateren1992}
\citation{Smith2006}
\citation{Bale2015}
\citation{Srinivasan1982,Brenner2000}
\citation{Brenner2000b}
\citation{Salinas2001,Averbeck2006}
\citation{Hubel1962}
\citation{Georgopoulos1986}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section.1}}
\citation{Mainen1995}
\citation{Shadlen1998}
\citation{VanVreeswijk1996,Renart2010}
\citation{Deneve2016,Boerlin2013}
\citation{Boerlin2013}
\citation{Barrett2013}
\citation{Barrett2015}
\citation{Brendel2016}
\citation{Deneve2016,Boerlin2013}
\@writefile{toc}{\contentsline {section}{\numberline {II}Derivations}{2}{section.2}}
\newlabel{sec:derive}{{II}{2}{Derivations}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {i}Assumptions}{2}{subsection.2.1}}
\citation{Stein1967}
\newlabel{eqn:loss}{{7}{3}{Assumptions}{equation.2.7}{}}
\newlabel{eqn:spiking}{{10}{3}{Assumptions}{equation.2.10}{}}
\newlabel{eqn:voltage}{{13}{3}{Assumptions}{equation.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {ii}Network dynamics}{3}{subsection.2.2}}
\newlabel{eqn:lifneuron}{{14}{3}{Network dynamics}{equation.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Encoding of a one-dimensional signal in a three-neuron network. \textbf  {(A)} Schematic of optimal balanced networks. Steady-state membrane potentials of the neurons in the network receive balanced input from the encoding $\textbf  {F}^*\textbf  {x}(t)$ and decoding signal $\textbf  {W}^*\textbf  {r}(t)$. Thus, membrane potentials represent prediction errors of the reconstructed signal. \textbf  {(B)} Step function (red) and decoding signal (blue) in a single trial. The membrane potential and spike trains for each of the three neurons are shown in separate colors and rows. \textbf  {(C)} Random signal as input, otherwise same as \textbf  {(B)}. Note the trade-off between decoding accuracy and overall numbers of spikes emitted. \textbf  {(D}--\textbf  {E)} Interspike interval (ISI) distribution for population \textbf  {(D)} and single-neuron \textbf  {(E)} spike trains.\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1Dsys}{{1}{4}{Encoding of a one-dimensional signal in a three-neuron network. \textbf {(A)} Schematic of optimal balanced networks. Steady-state membrane potentials of the neurons in the network receive balanced input from the encoding $\b {F}^*\b {x}(t)$ and decoding signal $\b {W}^*\b {r}(t)$. Thus, membrane potentials represent prediction errors of the reconstructed signal. \textbf {(B)} Step function (red) and decoding signal (blue) in a single trial. The membrane potential and spike trains for each of the three neurons are shown in separate colors and rows. \textbf {(C)} Random signal as input, otherwise same as \textbf {(B)}. Note the trade-off between decoding accuracy and overall numbers of spikes emitted. \textbf {(D}--\textbf {E)} Interspike interval (ISI) distribution for population \textbf {(D)} and single-neuron \textbf {(E)} spike trains.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {iii}Learning of recurrent connectivity}{4}{subsection.2.3}}
\newlabel{eqn:learn}{{19}{4}{Learning of recurrent connectivity}{equation.2.19}{}}
\newlabel{eqn:learncost}{{20}{4}{Learning of recurrent connectivity}{equation.2.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Encoding of a two-dimensional, circular signal in a network of $N=16$ neurons using a quadratic cost factor of $\mu =0.1$. \textbf  {(A)} Input signal $\textbf  {x}(t)$ and decoding signal $\textbf  {y}(t)$ for $T=10$ ~s. After only a few time steps, the coding error $\delimiter "026B30D (\textbf  {x}(t)-\textbf  {y}(t) \delimiter "026B30D ^2$ between input and decoder is bounded by $\Theta =\frac  {\delimiter "026B30D \textbf  {D}\delimiter "026B30D }{2}$. \textbf  {(B)} Feed-forward encoding and decoding weight vectors $(\textbf  {F}^T)_i$ and $\textbf  {D}_i$, respectively, in signal space. The edgecolor of the encoding vectors are black, while the decoding vectors are indicated by a gray edgecolor. Each neuron is color-coded using the Matplotlib colormap \textsc  {''winter''}. The same color code is used in \textbf  {(C}--\textbf  {D)}. \textbf  {(C)} Spike raster and firing rates for simulating the network for $T=30$ s. The spike pattern reflects the periodic nature of the incoming signal. The population rate $R(t) = \frac  {\DOTSB \sum@ \slimits@ _i^Nr_i(t)}{N}$ is indicated by the black solid line. Note that, while single neuron spiking is variable, the population rate is reliably constant and is proportional to the amplitude of the input signal. \textbf  {(D)} Numerical estimation of tuning curves of the network using univariate spline interpolation of firing rates over $T=500$~s. \relax }}{5}{figure.caption.2}}
\newlabel{fig:2Dsys}{{2}{5}{Encoding of a two-dimensional, circular signal in a network of $N=16$ neurons using a quadratic cost factor of $\mu =0.1$. \textbf {(A)} Input signal $\b {x}(t)$ and decoding signal $\b {y}(t)$ for $T=10$ ~s. After only a few time steps, the coding error $\|(\b {x}(t)-\b {y}(t) \|^2$ between input and decoder is bounded by $\Theta =\frac {\|\b {D}\|}{2}$. \textbf {(B)} Feed-forward encoding and decoding weight vectors $(\b {F}^T)_i$ and $\b {D}_i$, respectively, in signal space. The edgecolor of the encoding vectors are black, while the decoding vectors are indicated by a gray edgecolor. Each neuron is color-coded using the Matplotlib colormap \textsc {''winter''}. The same color code is used in \textbf {(C}--\textbf {D)}. \textbf {(C)} Spike raster and firing rates for simulating the network for $T=30$ s. The spike pattern reflects the periodic nature of the incoming signal. The population rate $R(t) = \frac {\sum _i^Nr_i(t)}{N}$ is indicated by the black solid line. Note that, while single neuron spiking is variable, the population rate is reliably constant and is proportional to the amplitude of the input signal. \textbf {(D)} Numerical estimation of tuning curves of the network using univariate spline interpolation of firing rates over $T=500$~s. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Simulations}{5}{section.3}}
\newlabel{sec:results}{{III}{5}{Simulations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {i}Learning optimal recurrent weights}{6}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Effects of different quadratic cost factors $\mu $ on mean population firing rates, their variances, and mean squared coding errors. We varied $\mu $ with values $0.0$, $0.01$, $0.02$, $0.1$, $0.2$, $0.5$, $1$, $2$, $5$, $10$, which are individually color-coded. The gray solid line indicates the mean values for each value of $\mu $.\relax }}{6}{figure.caption.3}}
\newlabel{fig:mu}{{3}{6}{Effects of different quadratic cost factors $\mu $ on mean population firing rates, their variances, and mean squared coding errors. We varied $\mu $ with values $0.0$, $0.01$, $0.02$, $0.1$, $0.2$, $0.5$, $1$, $2$, $5$, $10$, which are individually color-coded. The gray solid line indicates the mean values for each value of $\mu $.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Learning optimal recurrent connectivity in a network of $N=16$ neurons using a quadratic cost factor of $\mu =0.01$. \textbf  {(A)} Input signal $\textbf  {x}(t)$ and decoding signal $\textbf  {y}(t)$ during the initial phase of learning at $t=0 - 50$~s (top) and final phase of learning at $t = 150-200$~s (bottom). First, the decoder is overshooting the signal reflecting suboptimal recurrent weights, which favor excitation in the network. After learning, the coding error is bounded like in the optimal network in Fig.~\ref  {fig:2Dsys}. \textbf  {(B)} Feed-forward encoding and decoding weight vectors $(\textbf  {F}^T)_i$ and $\textbf  {D}_i$, respectively, in signal space. The edgecolor of the encoding vectors are black, while the decoding vectors are indicated by a gray edgecolor. We used the same color code as in Fig. \ref  {fig:2Dsys}. \textbf  {(C}--\textbf  {D)} Recurrent weight matrix, spike trains and firing rates with respect to time for \textbf  {(C)} $t=0-50$ s and \textbf  {(D)} $t=450-500$ s. The population rate is shown as a yellow solid line.\relax }}{7}{figure.caption.4}}
\newlabel{fig:Learn}{{4}{7}{Learning optimal recurrent connectivity in a network of $N=16$ neurons using a quadratic cost factor of $\mu =0.01$. \textbf {(A)} Input signal $\b {x}(t)$ and decoding signal $\b {y}(t)$ during the initial phase of learning at $t=0 - 50$~s (top) and final phase of learning at $t = 150-200$~s (bottom). First, the decoder is overshooting the signal reflecting suboptimal recurrent weights, which favor excitation in the network. After learning, the coding error is bounded like in the optimal network in Fig.~\ref {fig:2Dsys}. \textbf {(B)} Feed-forward encoding and decoding weight vectors $(\b {F}^T)_i$ and $\b {D}_i$, respectively, in signal space. The edgecolor of the encoding vectors are black, while the decoding vectors are indicated by a gray edgecolor. We used the same color code as in Fig. \ref {fig:2Dsys}. \textbf {(C}--\textbf {D)} Recurrent weight matrix, spike trains and firing rates with respect to time for \textbf {(C)} $t=0-50$ s and \textbf {(D)} $t=450-500$ s. The population rate is shown as a yellow solid line.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {ii}Optimal balance and recurrent plasticity for compensating neuronal death at constant rate}{7}{subsection.3.2}}
\newlabel{eqn:pseudo}{{24}{7}{Optimal balance and recurrent plasticity for compensating neuronal death at constant rate}{equation.3.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of networks with and without recurrent plasticity during constant-rate neuronal death. \textbf  {(A}--\textbf  {B)} Input signal $\textbf  {x}(t)$ and decoding signal $\textbf  {y}(t)$ during the final $50$ s, as well as the final encoding and decoding weight vectors for the \textbf  {(A)} plastic and \textbf  {(B)} static network. The color code is similar to the previous figures. \textbf  {(C)} Recurrent weight matrix with respect to time. The time difference between each display is $\Delta t = 100$ s to show effects of individual ablations. \textbf  {(D)} Coding distance and population rates for the plastic and static network. We measure coding accuracy by using the distance $\delimiter "026B30D \textbf  {x}-\textbf  {y}\delimiter "026B30D $ between input and decoding signal, which is lower for the plastic network (i.e., better coding accuracy). We show that recurrent plasticity reduces the variance of the population rate by equalizing the increase of certain firing rates due to acute optimal balance compensation.\relax }}{8}{figure.caption.5}}
\newlabel{fig:recovery}{{5}{8}{Comparison of networks with and without recurrent plasticity during constant-rate neuronal death. \textbf {(A}--\textbf {B)} Input signal $\b {x}(t)$ and decoding signal $\b {y}(t)$ during the final $50$ s, as well as the final encoding and decoding weight vectors for the \textbf {(A)} plastic and \textbf {(B)} static network. The color code is similar to the previous figures. \textbf {(C)} Recurrent weight matrix with respect to time. The time difference between each display is $\Delta t = 100$ s to show effects of individual ablations. \textbf {(D)} Coding distance and population rates for the plastic and static network. We measure coding accuracy by using the distance $\|\b {x}-\b {y}\|$ between input and decoding signal, which is lower for the plastic network (i.e., better coding accuracy). We show that recurrent plasticity reduces the variance of the population rate by equalizing the increase of certain firing rates due to acute optimal balance compensation.\relax }{figure.caption.5}{}}
\citation{Brendel2016}
\bibdata{report}
\bibcite{Marr1982}{{1}{1982}{{Marr}}{{}}}
\bibcite{Adrian1926}{{2}{1926}{{Adrian and Zotterman}}{{}}}
\bibcite{Ernst2002}{{3}{2002}{{Ernst and Banks}}{{}}}
\bibcite{Behrens2007}{{4}{2007}{{Behrens et~al.}}{{Behrens, Woolrich, Walton, and Rushworth}}}
\bibcite{Drugowitsch2014}{{5}{2014}{{Drugowitsch et~al.}}{{Drugowitsch, DeAngelis, Klier, Angelaki, and Pouget}}}
\bibcite{Attneave1954}{{6}{1954}{{Attneave}}{{}}}
\bibcite{Barlow1961}{{7}{1961}{{Barlow}}{{}}}
\bibcite{Shannon1948}{{8}{1948}{{Shannon}}{{}}}
\bibcite{Olshausen1996}{{9}{1996}{{Olshausen and Field}}{{}}}
\bibcite{Bell1997}{{10}{1997}{{Bell and Sejnowski}}{{}}}
\bibcite{Atick1992}{{11}{1992}{{Atick and Redlich}}{{}}}
\bibcite{Pitkow2012}{{12}{2012}{{Pitkow and Meister}}{{}}}
\bibcite{Srinivasan1982}{{13}{1982}{{Srinivasan et~al.}}{{Srinivasan, Laughlin, and Dubs}}}
\bibcite{VanHateren1992}{{14}{1992}{{Van~Hateren}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Conclusions}{9}{section.4}}
\newlabel{sec:discuss}{{IV}{9}{Conclusions}{section.4}{}}
\bibcite{Smith2006}{{15}{2006}{{Smith and Lewicki}}{{}}}
\bibcite{Bale2015}{{16}{2015}{{Bale et~al.}}{{Bale, Ince, Santagata, and Petersen}}}
\bibcite{Brenner2000}{{17}{2000{}}{{Brenner et~al.}}{{Brenner, Bialek, and Van~Steveninck}}}
\bibcite{Brenner2000b}{{18}{2000{}}{{Brenner et~al.}}{{Brenner, Strong, Koberle, Bialek, and Van~Steveninck}}}
\bibcite{Salinas2001}{{19}{2001}{{Salinas and Sejnowski}}{{}}}
\bibcite{Averbeck2006}{{20}{2006}{{Averbeck et~al.}}{{Averbeck, Latham, and Pouget}}}
\bibcite{Hubel1962}{{21}{1962}{{Hubel and Wiesel}}{{}}}
\bibcite{Georgopoulos1986}{{22}{1986}{{Georgopoulos et~al.}}{{Georgopoulos, Schwartz, Kettner, et~al.}}}
\bibcite{Mainen1995}{{23}{1995}{{Mainen and Sejnowski}}{{}}}
\bibcite{Shadlen1998}{{24}{1998}{{Shadlen and Newsome}}{{}}}
\bibcite{VanVreeswijk1996}{{25}{1996}{{van Vreeswijk and Sompolinsky}}{{}}}
\bibcite{Renart2010}{{26}{2010}{{Renart et~al.}}{{Renart, De~La~Rocha, Bartho, Hollender, Parga, Reyes, and Harris}}}
\bibcite{Deneve2016}{{27}{2016}{{Den{\`e}ve and Machens}}{{}}}
\bibcite{Boerlin2013}{{28}{2013}{{Boerlin et~al.}}{{Boerlin, Machens, and Den{\`e}ve}}}
\bibcite{Barrett2013}{{29}{2013}{{Barrett et~al.}}{{Barrett, Den{\`e}ve, and Machens}}}
\bibcite{Barrett2015}{{30}{2015}{{Barrett et~al.}}{{Barrett, Deneve, and Machens}}}
\bibcite{Brendel2016}{{31}{2016}{{Brendel et~al.}}{{Brendel, Bourdoukan, Vertechi, Barrett, Machens, and Den\`{e}ve}}}
\bibcite{Stein1967}{{32}{1967}{{Stein}}{{}}}
\newlabel{eqn:project}{{26}{11}{}{equation.4.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Examples of (A-B) bounded and (C-D) unbounded error conditions for the 3-neuron network in two dimensions.\relax }}{12}{figure.caption.6}}
\newlabel{fig:N3}{{6}{12}{Examples of (A-B) bounded and (C-D) unbounded error conditions for the 3-neuron network in two dimensions.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Examples of multiple spikes in the 3-neuron network for two dimensions. Depending on a given error, the neuron can spike (A) one, (B) two, or (C) more times to reach inside the error bound. (D) Assuming that the error dynamics are slow, i.e., the error slowly reaches the threshold, one can illustrate possible effects of a spike by a rectangle spanned by $-\textbf  {D}_i$. The probability of exhibiting two spikes is therefore given by the ratio of areas outside vs. inside the error bound.\relax }}{12}{figure.caption.7}}
\newlabel{fig:multispike}{{7}{12}{Examples of multiple spikes in the 3-neuron network for two dimensions. Depending on a given error, the neuron can spike (A) one, (B) two, or (C) more times to reach inside the error bound. (D) Assuming that the error dynamics are slow, i.e., the error slowly reaches the threshold, one can illustrate possible effects of a spike by a rectangle spanned by $-\b {D}_i$. The probability of exhibiting two spikes is therefore given by the ratio of areas outside vs. inside the error bound.\relax }{figure.caption.7}{}}
